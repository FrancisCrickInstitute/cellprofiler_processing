# Cell Painting Data Processing Pipeline

Python pipeline for processing CellProfiler output with comprehensive feature selection, normalization, dimensionality reduction, landmark analysis, and hierarchical clustering.

## Table of Contents
- [Requirements](#requirements)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Pipeline Modes](#pipeline-modes)
- [CellProfiler Data Source](#cellprofiler-data-source)
- [Input Files](#input-files)
- [Configuration File](#configuration-file)
- [Processing Pipeline](#processing-pipeline)
- [Normalization Methods](#normalization-methods)
- [Landmark Analysis](#landmark-analysis)
- [Hierarchical Clustering](#hierarchical-clustering)
- [Output Structure](#output-structure)
- [Usage Examples](#usage-examples)
- [Command Line Options](#command-line-options)
- [Troubleshooting](#troubleshooting)

## Requirements

**Python >= 3.8**

See `environment.yml` for complete package requirements.

**Key dependencies:**
- `pandas`, `numpy`, `scikit-learn`, `scipy`, `pyyaml`, `tqdm`
- `plotly`, `matplotlib`, `seaborn`, `umap-learn`
- `morar` (for Z-score normalization)

## Installation

```bash
conda env create -f environment.yml
conda activate cellprofiler_analysis
```

## Quick Start

```bash
# Full pipeline from raw data
sbatch submit.sh full

# From well-level data (faster iteration)
sbatch submit.sh well /path/to/previous/run

# Regenerate plots only
sbatch submit.sh replot /path/to/previous/run
```

Or run directly:
```bash
python -m cellprofiler_analyser.main \
    --config config.yml \
    --start-from full
```

### Example Files

The latest example configuration and submission scripts:

| File | Description |
|------|-------------|
| `cellprofiler_analyser/config/config_20251111_gsk_prosperity_all.yml` | **Latest config** - Full example with all options |
| `cellprofiler_analyser/submit/submit_20251111_gsk_prosperity_all_flexible.sh` | **Latest submit script** - Three-mode SLURM submission |

Use these as templates for your own analyses.

---

## Pipeline Modes

The pipeline supports **three entry points** for different use cases:

### Mode Comparison

| Feature                     | full Mode              | well Mode              | replot Mode            |
|-----------------------------|------------------------|------------------------|------------------------|
| Raw data processing         | ✓ Always               | ✗ Skipped              | ✗ Skipped              |
| UMAP/t-SNE computation      | ✓ or skip (YAML)       | ✓ or skip (YAML)       | ✗ Skipped              |
| UMAP/t-SNE plotting         | ✓ (note 1)             | ✓ or skip (YAML)       | ✓ From coordinates     |
| Landmark analysis           | Optional (YAML)        | Optional (YAML)        | ✗ NOT regenerated      |
| Hierarchical clustering     | Optional (note 2)      | Optional (note 2)      | ✗ NOT regenerated      |
| Landmark threshold analysis | Optional (note 2)      | Optional (note 2)      | ✗ NOT regenerated      |
| Viz export (CSV)            | ✓ Generated            | ✓ Generated            | ✗ NOT regenerated      |
| Output folder               | `YYYYMMDD_results`     | `YYYYMMDD_from_well_*` | `YYYYMMDD_replot_*`    |

### Flag Dependencies (auto-handled)
- Clustering requires landmark analysis (auto-enabled if needed) **[note 2]**
- Threshold analysis requires landmark analysis (auto-enabled if needed) **[note 2]**
- `skip_embedding_generation` requires previous run coordinates
- replot mode requires previous run with embedding coordinates
- In full mode, UMAP/t-SNE plotting only runs when computation is enabled **[note 1]** (no previous coordinates available if computation is skipped)

### Mode 1: Full Pipeline
Complete analysis from raw CellProfiler data.

```bash
sbatch submit.sh full
```

**Use when:**
- First run on new data
- You need cell count values in final viz dataframe
- Starting fresh analysis

### Mode 2: Well-Level Entry
Start from pre-computed well-level data for faster iteration.

```bash
sbatch submit.sh well /path/to/previous/run
```

**Use when:**
- Testing different landmark parameters
- Re-running analysis with different MAD thresholds
- Iterating on clustering settings

### Mode 3: Replot Only
Regenerate UMAP/t-SNE plots from existing coordinates.

```bash
sbatch submit.sh replot /path/to/previous/run
```

**Use when:**
- Tweaking plot colors or styling
- Regenerating visualizations after code updates
- Quick visual checks

**Note:** This mode does NOT regenerate landmark analysis or clustering.

---

## CellProfiler Data Source

The `Image.parquet` file required as input is generated by running CellProfiler on your raw imaging data.

### CellProfiler Configuration

This configuration file controls HPC implementation of CellProfiler.

**Original source:** [Scott Warchal's CP Config Repo](https://github.com/FrancisCrickInstitute/cp_config)

Example configuration files are provided in the `cp_config/` directory:

- `cp_config.yml` - CellProfiler configuration
- `submit_analysis.sh` - SLURM script for running CellProfiler analysis
- `submit_collate.sh` - SLURM script for collating results

---

## Input Files

### 1. Image.parquet (required for full mode)
CellProfiler output containing:
- `ImageNumber` - Unique image identifier
- Feature columns: `Cells_*`, `Nuclei_*`, `Cytoplasm_*`
- File path columns: `URL_*` (used to extract plate/well/field info)

**Multiple input files supported:**
```yaml
input_files:
  - "/path/to/dataset1/Image.parquet"
  - "/path/to/dataset2/Image.parquet"
```

### 2. metadata.csv (recommended)
Perturbation metadata with **required** columns:
- `Metadata_lib_plate_order` - Plate library identifier
- `Metadata_well` - Well position (e.g., A01, B12)
- `Metadata_perturbation_name` - Treatment/compound name
- `Metadata_compound_uM` - Concentration in micromolar

**Optional but useful columns:**
- `Metadata_PP_ID` - Compound identifier
- `Metadata_SMILES` - Chemical structure
- Any other experimental metadata you want to track

**Note:** ALL columns from the metadata CSV will be merged into the final dataset.

### 3. config.yml (required)
Unified configuration file controlling all pipeline behavior. See [Configuration File](#configuration-file) section below.

---

## Configuration File

The `config.yml` file controls all aspects of the pipeline. Here's a complete annotated example:

```yaml
# ============================================================================
# INPUT/OUTPUT PATHS
# ============================================================================
input_files:
  - "/path/to/Image.parquet"

metadata_file: "/path/to/metadata.csv"

output_base_dir: "/path/to/processed_data"

# For well/replot modes - reference to previous run
skip_mode_paths:
  previous_run_base: "/path/to/previous/YYYYMMDD_results"
  well_level_parquet: "data/processed_image_data_well_level.parquet"
  embedding_coordinates: "visualizations/coordinates/embedding_coordinates.csv"

# ============================================================================
# ANALYSIS FLAGS
# ============================================================================
analysis:
  run_landmark_analysis: true           # Identify landmark compounds
  run_hierarchical_clustering: true     # Generate clustering PDFs (requires landmarks)
  run_landmark_threshold_analysis: true # Test multiple MAD thresholds

# MAD thresholds to test (if threshold analysis enabled)
landmark_thresholds_to_analyze:
  - 0.10
  - 0.15
  - 0.20
  - 0.25
  - 0.30

# ============================================================================
# QUALITY CONTROL THRESHOLDS
# ============================================================================
quality_control:
  missing_threshold: 0.05           # Remove features with >5% missing values
  correlation_threshold: 0.95       # Remove features with >0.95 correlation
  high_variability_threshold: 15    # Remove features with CV > 15
  low_variability_threshold: 0.01   # Remove features with CV < 0.01

# ============================================================================
# NORMALIZATION SETTINGS
# ============================================================================
normalization:
  method: "z_score"                      # Method: "z_score" or "robust_mad"
  normalization_type: "control_based"    # "control_based" or "all_conditions"
  control_compound: "DMSO"               # Which compound to use as baseline
  control_column: "Metadata_perturbation_name"

# ============================================================================
# VISUALIZATION PARAMETERS
# ============================================================================
visualization:
  # Set to true to reuse existing UMAP/t-SNE coordinates from previous run
  # Only works in 'well' mode - requires embedding_coordinates from previous run
  skip_embedding_generation: false
  
  umap_parameters:
    - name: "n15_d0.1"
      n_neighbors: 15
      min_dist: 0.1
    - name: "n5_d0.01"
      n_neighbors: 5
      min_dist: 0.01
    - name: "n30_d0.1"
      n_neighbors: 30
      min_dist: 0.1
      
  tsne_parameters:
    - name: "p30"
      perplexity: 30
    - name: "p50"
      perplexity: 50

# ============================================================================
# LIBRARY DEFINITIONS (for landmark analysis)
# ============================================================================
library_definitions:
  test_libraries:
    - "HTC_V1"
    - "HTC_V2"
    - "GSK_fragments_V3"
  reference_libraries:
    - "JUMP_V1"
    - "JUMP_V2"
    - "GSK_chemogenetic_V1"
    - "CRISPR_genome"

# ============================================================================
# PLATE DEFINITIONS
# ============================================================================
plate_dict:
  "32084":                    # Plate barcode (extracted from file paths)
    lib_plate_order: 63       # Library plate number (must match metadata CSV)
    replicate: 1
    compound_uM: 0.1
    cell_type: "HaCaT"
    library: "GSK_lib"
    type: "reference"         # "reference" or "test" for landmark analysis
    
  "32085":
    lib_plate_order: 64
    replicate: 2
    compound_uM: 0.1
    cell_type: "HaCaT"
    library: "GSK_lib"
    type: "test"
```

---

## Processing Pipeline

The pipeline follows this order (in **full mode**):

### Step 1: Data Loading & Metadata Integration
1. Load `Image.parquet` file(s) - supports multiple inputs
2. Add `Metadata_` prefix to technical columns
3. Extract plate/well/field from file paths
4. Map plate metadata using `plate_dict`
5. Merge perturbation metadata from CSV
6. Create `Metadata_treatment` column (perturbation_name @ compound_uM)
7. Create `Metadata_PP_ID_uM` column (if PP_ID available)

### Step 2: Feature Selection (Image-level)
Starting features: ~1,800 → Target: ~500-800

**Removed features:**
- Missing values (>5% default)
- Blacklisted patterns (Location, Count, Edge, ExecutionTime)
- High correlation (>0.95 default)
- Low/high variance outliers

### Step 3: Z-score Normalization (Image-level)
Normalization performed **plate-wise**.

**Control-based (default):**
```
z_score = (value - DMSO_mean_per_plate) / DMSO_std_per_plate
```

**All-conditions:**
```
z_score = (value - all_mean_per_plate) / all_std_per_plate
```

### Step 4: Well-level Aggregation
- Method: Median of all field images per well
- Reduces ~100,000s images → ~10,000s wells

### Step 5: Dimensionality Reduction
- **PCA** - ~50 components explaining >80% variance
- **UMAP** - Multiple parameter sets from config
- **t-SNE** - Multiple parameter sets from config

### Step 6: Visualization Generation
- Interactive Plotly HTML plots
- Coordinates saved to CSV for fast replotting

### Step 7: Landmark Analysis (if enabled)
- Identify landmark compounds from reference plates
- Calculate distances to landmarks for test compounds

### Step 8: Hierarchical Clustering (if enabled)
- Requires landmark analysis
- Creates multi-page PDF heatmaps

### Step 9: Visualization Export
- Creates `cp_for_viz_app.csv` with metadata + coordinates + landmark info

---

## Normalization Methods

| Baseline | Use Case | Result |
|----------|----------|--------|
| `control_based` (DMSO) | Default. Shows treatment effects relative to controls | DMSO = 0, treatments show deviation |
| `all_conditions` | Compare relative positions across all conditions | All conditions = 0, shows global patterns |

Override via command line:
```bash
--normalization-baseline dmso
--normalization-baseline all-conditions
```

---

## Landmark Analysis

### What Are Landmarks?

**Landmarks** are reference compounds with:
1. High reproducibility (low MAD across replicates)
2. Strong phenotypic signal (high distance from DMSO)
3. Serve as "anchors" in phenotypic space

### How It Works

1. **Identify landmarks** from reference plates (`type: "reference"`)
   - Calculate MAD (Median Absolute Deviation) for each compound
   - Calculate distance from DMSO controls
   - Select compounds meeting threshold criteria

2. **Compare test compounds** to identified landmarks
   - Calculate distances from test plates to landmarks
   - Generate similarity matrices

### Required Config

```yaml
analysis:
  run_landmark_analysis: true

plate_dict:
  "32084":
    type: "reference"    # Used to identify landmarks
  "32085":
    type: "test"         # Compared against landmarks
```

---

## Hierarchical Clustering

Hierarchical clustering runs **only when landmark analysis is enabled** (they share distance matrices).

### Outputs

```
hierarchical_clustering/
├── clustering_information.csv
├── linkage_matrix.csv
├── ordered_cosine_similarity_matrix.parquet
├── reference_landmarks/
│   └── reference_landmarks_all_chunks.pdf
├── reference_only/
│   └── reference_only_all_chunks.pdf
├── test_and_all_reference_landmarks/
│   └── test_and_all_reference_landmarks_all_chunks.pdf
├── test_and_reference/
│   └── test_and_reference_all_chunks.pdf
├── test_only/
│   └── test_only_all_chunks.pdf
└── test_valid_and_relevant_landmarks/
    └── test_valid_and_relevant_landmarks_all_chunks.pdf
```

**What are "chunks"?**
Large heatmaps are split into multi-page PDFs for readability.

---

## Output Structure

```
output_dir/YYYYMMDD_HHMMSS_results/
├── data/
│   ├── processed_image_data.parquet
│   ├── processed_image_data_normalized.parquet
│   ├── processed_image_data_standardscaler_scaled.parquet
│   ├── processed_image_data_well_level.parquet
│   ├── processed_image_data_well_level.csv
│   ├── cp_for_viz_app.csv              # Final viz export
│   └── cp_for_viz_app.parquet
│
├── samples/
│   ├── processed_image_data_sample_10000.csv
│   ├── processed_image_data_normalized_sample_10000.csv
│   ├── processed_image_data_standardscaler_scaled_sample_10000.csv
│   └── processed_image_data_well_level_sample_10000.csv
│
├── intermediate/
│   ├── feature_selected_data.parquet
│   ├── feature_selected_info.txt
│   ├── feature_selected_sample.csv
│   ├── normalization_info_all_conditions.txt
│   ├── normalized_data_all_conditions.parquet
│   ├── well_aggregated_data_all_conditions.parquet
│   └── well_aggregated_sample_all_conditions.csv
│
├── analysis/
│   ├── pca/
│   │   ├── pca_feature_loadings.csv
│   │   ├── pca_variance_analysis.png
│   │   ├── pca_variance_data.csv
│   │   └── pca_variance_explained.csv
│   ├── correlation/
│   │   ├── correlation_heatmap_all_features.png
│   │   └── correlation_matrix.csv
│   └── histograms/
│       ├── normalized/
│       └── raw/
│
├── visualizations/
│   ├── umap/interactive/*.html
│   ├── tsne/interactive/*.html
│   └── coordinates/
│       └── embedding_coordinates.csv
│
├── visualizations_redo/                # Regenerated plots (well/replot modes)
│   ├── umap/
│   └── tsne/
│
├── landmark_analysis/                  # If enabled
│   ├── cellprofiler_landmarks.csv
│   ├── reference_mad_and_dmso.csv
│   ├── test_mad_and_dmso.csv
│   ├── reference_to_landmark_distances.csv
│   ├── test_to_landmark_distances.csv
│   ├── reference_centroids.parquet
│   ├── test_centroids.parquet
│   ├── reference_distances.parquet
│   ├── reference_distances_sample.csv
│   ├── test_distances.parquet
│   ├── test_distances_sample.csv
│   ├── landmark_metadata.parquet
│   ├── landmark_metadata_sample.csv
│   ├── treatment_metadata_for_clustering.csv
│   ├── cosine_distance_matrix_for_clustering.parquet
│   └── cosine_similarity_matrix_for_clustering.parquet
│
├── hierarchical_clustering/            # If enabled
│   ├── clustering_information.csv
│   ├── linkage_matrix.csv
│   ├── ordered_cosine_similarity_matrix.parquet
│   ├── reference_landmarks/
│   │   └── reference_landmarks_all_chunks.pdf
│   ├── reference_only/
│   │   └── reference_only_all_chunks.pdf
│   ├── test_and_all_reference_landmarks/
│   │   └── test_and_all_reference_landmarks_all_chunks.pdf
│   ├── test_and_reference/
│   │   └── test_and_reference_all_chunks.pdf
│   ├── test_only/
│   │   └── test_only_all_chunks.pdf
│   └── test_valid_and_relevant_landmarks/
│       └── test_valid_and_relevant_landmarks_all_chunks.pdf
│
├── landmark_threshold_analysis/        # If enabled
│   ├── by_library/
│   ├── comparisons/
│   ├── distributions/
│   ├── summary_statistics.csv
│   └── threshold_counts_per_treatment.csv
│
└── comprehensive_summary.txt
```
```

---

## Usage Examples

### 1. Full Pipeline
```bash
sbatch submit.sh full
```

### 2. From Well-Level Data
```bash
sbatch submit.sh well /path/to/previous/run
```

### 3. Replot Only
```bash
sbatch submit.sh replot /path/to/previous/run
```

### 4. Direct Python Execution
```bash
python -m cellprofiler_analyser.main \
    --config config.yml \
    --start-from full
```

### 5. Override Thresholds
```bash
python -m cellprofiler_analyser.main \
    --config config.yml \
    --start-from full \
    --missing-threshold 0.10 \
    --correlation-threshold 0.90
```

### 6. Skip Landmark Threshold Analysis
```bash
python -m cellprofiler_analyser.main \
    --config config.yml \
    --start-from well \
    --previous-run-dir /path/to/previous/run \
    --skip-landmark-threshold-analysis
```

---

## Command Line Options

### Mode Selection
- `--start-from` - Pipeline entry point: `full`, `well`, or `replot`
- `--previous-run-dir` - Path to previous run (for well/replot modes)

### Input/Output
- `--input`, `-i` - Override input file(s) from config
- `--metadata`, `-m` - Metadata CSV file
- `--config`, `-c` - Config YAML file (required)
- `--output`, `-o` - Output directory
- `--no-timestamp` - Disable automatic timestamp in output directory

### Analysis Flags
- `--run-landmark-analysis` - Enable landmark analysis
- `--run-hierarchical-clustering` - Enable hierarchical clustering
- `--run-landmark-threshold-analysis` - Enable threshold analysis
- `--skip-landmark-threshold-analysis` - Skip threshold analysis

### Normalization
- `--normalization-baseline` - Override: `dmso` or `all-conditions`

### Quality Thresholds
- `--missing-threshold` - Missing value threshold (default: 0.05)
- `--correlation-threshold` - Correlation threshold (default: 0.95)
- `--high-var-threshold` - High variability threshold (default: 15.0)
- `--low-var-threshold` - Low variability threshold (default: 0.01)

### Other
- `--verbose`, `-v` - Enable verbose logging
- `--no-pca` - Don't load PCA model (replot mode)

---

## Troubleshooting

### "morar package not found"
```bash
pip install morar
```

### "Missing required columns in metadata"
Your metadata CSV must have:
- `Metadata_lib_plate_order`
- `Metadata_well`

### Landmark analysis failed
Ensure your config has `type: "reference"` and `type: "test"` for appropriate plates.

### Empty directories in output
This is a known issue when running well/replot modes. Empty `analysis/`, `visualizations/`, `visualizations_redo/` directories may appear in the parent folder - these can be safely deleted.

### Memory error on HPC
Increase SLURM memory:
```bash
#SBATCH --mem=1024G
```

### No UMAP/t-SNE plots generated
- In **full mode**: Check that `skip_embedding_generation` is false
- In **well mode**: Ensure coordinates file exists in previous run
- Check logs for errors in `recreate_plots_from_coordinates()`

### Hierarchical clustering not running
Requires `run_landmark_analysis: true` in config (clustering depends on landmark distance matrices).

---

## Typical Processing Statistics

| Metric | Value |
|--------|-------|
| Input | ~100,000 images across 10+ plates |
| Feature selection | 1,800 → 650 features |
| Well aggregation | 100,000 images → ~4,000 wells |
| Dimensionality reduction | PCA (50) + UMAP + t-SNE |
| Output | ~50 interactive HTML plots |
| Time (full mode) | ~2-4 hours on HPC (512GB RAM) |
| Time (well mode) | ~30-60 minutes |
| Time (replot mode) | ~5-10 minutes |

---

## Questions or Issues?

1. Check `comprehensive_summary.txt` for processing details
2. Review config file structure
3. Ensure plate barcodes in `plate_dict` match file paths exactly
4. Verify `lib_plate_order` matches metadata CSV

---

## Version History

- **v2.0** - Three-mode pipeline (full/well/replot), unified YAML config, landmark threshold analysis
- **v1.0** - Initial release with basic processing and clustering


## Related Repositories

- [spc-distributed](https://github.com/FrancisCrickInstitute/spc-distributed)
- [spc-data-explorer](https://github.com/FrancisCrickInstitute/spc-data-explorer) - Interactive visualization dashboard
- [spc-cosine-analysis](https://github.com/FrancisCrickInstitute/spc-cosine-analysis) - SPC-based analysis pipeline